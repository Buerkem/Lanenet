{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8467dcf6"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
        "from tensorflow.keras.layers import Add\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from tensorflow.keras.layers import Activation, Input\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import BatchNormalization, Concatenate\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Softmax, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import skimage\n",
        "from skimage.io import imread"
      ],
      "id": "8467dcf6"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d81d6f94"
      },
      "outputs": [],
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    Arguments:\n",
        "        inputs (tensor): Input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): Activation name\n",
        "        batch_normalization (bool): Whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    Returns:\n",
        "        x (tensor): Tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "id": "d81d6f94"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4e096c02"
      },
      "outputs": [],
      "source": [
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): Shape of input image tensor\n",
        "        depth (int): Number of core convolutional layers\n",
        "        num_classes (int): Number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = Add()([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # feature maps\n",
        "    outputs = features_pyramid(x, n_layers)\n",
        "    \n",
        "\n",
        "    # instantiate model\n",
        "    name = 'ResNet%dv1' % (depth)\n",
        "    model = Model(inputs=inputs,\n",
        "                  outputs=outputs,\n",
        "                  name=name)\n",
        "    return model"
      ],
      "id": "4e096c02"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "20a8cbb3"
      },
      "outputs": [],
      "source": [
        "def resnet_v2(input_shape, depth, n_layers=4):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): Shape of input image tensor\n",
        "        depth (int): Number of core convolutional layers\n",
        "        num_classes (int): Number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = Add()([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    # 1st feature map layer\n",
        "\n",
        "    # main feature maps (160, 120)\n",
        "    # succeeding feature maps are scaled down by\n",
        "    # 2, 4, 8\n",
        "    outputs = features_pyramid(x, n_layers)\n",
        "\n",
        "    # instantiate model.\n",
        "    name = 'ResNet%dv2' % (depth)\n",
        "    model = Model(inputs=inputs,\n",
        "                  outputs=outputs,\n",
        "                  name=name)\n",
        "    return model"
      ],
      "id": "20a8cbb3"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1a2f7765"
      },
      "outputs": [],
      "source": [
        "def features_pyramid(x, n_layers):\n",
        "    \"\"\"Generate features pyramid from the output of the \n",
        "    last layer of a backbone network (e.g. ResNetv1 or v2)\n",
        "\n",
        "    Arguments:\n",
        "        x (tensor): Output feature maps of a backbone network\n",
        "        n_layers (int): Number of additional pyramid layers\n",
        "\n",
        "    Return:\n",
        "        outputs (list): Features pyramid \n",
        "    \"\"\"\n",
        "    outputs = [x]\n",
        "    conv = AveragePooling2D(pool_size=2, name='pool1')(x)\n",
        "    outputs.append(conv)\n",
        "    prev_conv = conv\n",
        "    n_filters = 512\n",
        "\n",
        "    # additional feature map layers\n",
        "    for i in range(n_layers - 1):\n",
        "        postfix = \"_layer\" + str(i+2)\n",
        "        conv = conv_layer(prev_conv,\n",
        "                          n_filters,\n",
        "                          kernel_size=3,\n",
        "                          strides=2,\n",
        "                          use_maxpool=False,\n",
        "                          postfix=postfix)\n",
        "        outputs.append(conv)\n",
        "        prev_conv = conv\n",
        "\n",
        "    return outputs"
      ],
      "id": "1a2f7765"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5002d251"
      },
      "outputs": [],
      "source": [
        "def build_resnet(input_shape,\n",
        "                 n_layers=4,\n",
        "                 version=2,\n",
        "                 n=6):\n",
        "    \"\"\"Build a resnet as backbone\n",
        "\n",
        "    # Arguments:\n",
        "        input_shape (list): Input image size and channels\n",
        "        n_layers (int): Number of feature layers \n",
        "        version (int): Supports ResNetv1 and v2 but v2 by default\n",
        "        n (int): Determines number of ResNet layers\n",
        "                 (Default is ResNet50)\n",
        "\n",
        "    # Returns\n",
        "        model (Keras Model)\n",
        "\n",
        "    \"\"\"\n",
        "    # computed depth from supplied model parameter n\n",
        "    if version == 1:\n",
        "        depth = n * 6 + 2\n",
        "    elif version == 2:\n",
        "        depth = n * 9 + 2\n",
        "\n",
        "    # model name, depth and version\n",
        "    # input_shape (h, w, 3)\n",
        "    if version==1:\n",
        "        model = resnet_v1(input_shape=input_shape,\n",
        "                          depth=depth,\n",
        "                          n_layers=n_layers)\n",
        "    else:\n",
        "        model = resnet_v2(input_shape=input_shape,\n",
        "                          depth=depth,\n",
        "                          n_layers=n_layers)\n",
        "    return model"
      ],
      "id": "5002d251"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "48f2c2a4"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def data_generation(keys, data_path, label_path):\n",
        "    \"\"\"Generate train data: images and \n",
        "    segmentation ground truth labels \n",
        "\n",
        "    Arguments:\n",
        "        keys (array): Randomly sampled keys\n",
        "            (key is image filename)\n",
        "\n",
        "    Returns:\n",
        "        x (tensor): Batch of images\n",
        "        y (tensor): Batch of pixel-wise categories\n",
        "    \"\"\"\n",
        "    # a batch of images\n",
        "    x = []\n",
        "    # and their corresponding segmentation masks\n",
        "    y = []\n",
        "\n",
        "    for i, key in enumerate(keys):\n",
        "        # images are assumed to be stored \n",
        "        # in self.args.data_path\n",
        "        # key is the image filename \n",
        "        image_path = os.path.join(data_path, key)\n",
        "        # load the image using OpenCV\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "        # resize the image to (640, 480)\n",
        "        image = cv2.resize(image, (640, 480), interpolation=cv2.INTER_LINEAR)\n",
        "        # convert the image from BGR to RGB format\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        # append image to the list\n",
        "        x.append(image)\n",
        "        # and its corresponding label (segmentation mask)\n",
        "        print('label_path: ', label_path)\n",
        "        label = os.path.join(label_path, key)\n",
        "        label = skimage.img_as_float(imread(label))\n",
        "        # resize the label to (640, 480)\n",
        "        label = cv2.resize(label, (640, 480), interpolation=cv2.INTER_NEAREST)\n",
        "        y.append(label)\n",
        "\n",
        "    return np.array(x), np.array(y)"
      ],
      "id": "48f2c2a4"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "93abef89"
      },
      "outputs": [],
      "source": [
        "def conv_layer(inputs,\n",
        "               filters=32,\n",
        "               kernel_size=3,\n",
        "               strides=1,\n",
        "               use_maxpool=True,\n",
        "               postfix=None,\n",
        "               activation=None):\n",
        "    \"\"\"Helper function to build Conv2D-BN-ReLU layer\n",
        "        with optional MaxPooling2D.\n",
        "    \"\"\"\n",
        "\n",
        "    x = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               strides=strides,\n",
        "               kernel_initializer='he_normal',\n",
        "               name=\"conv_\"+postfix,\n",
        "               padding='same')(inputs)\n",
        "    x = BatchNormalization(name=\"bn_\"+postfix)(x)\n",
        "    x = Activation('relu', name='relu_'+postfix)(x)\n",
        "    if use_maxpool:\n",
        "        x = MaxPooling2D(name='pool'+postfix)(x)\n",
        "    return x"
      ],
      "id": "93abef89"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2dd160f4"
      },
      "outputs": [],
      "source": [
        "def tconv_layer(inputs,\n",
        "                filters=32,\n",
        "                kernel_size=3,\n",
        "                strides=2,\n",
        "                postfix=None):\n",
        "    \"\"\"Helper function to build Conv2DTranspose-BN-ReLU \n",
        "        layer\n",
        "    \"\"\"\n",
        "    x = Conv2DTranspose(filters=filters,\n",
        "                        kernel_size=kernel_size,\n",
        "                        strides=strides,\n",
        "                        padding='same',\n",
        "                        kernel_initializer='he_normal',\n",
        "                        name='tconv_'+postfix)(inputs)\n",
        "    x = BatchNormalization(name=\"bn_\"+postfix)(x)\n",
        "    x = Activation('relu', name='relu_'+postfix)(x)\n",
        "    return x"
      ],
      "id": "2dd160f4"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "609d0456"
      },
      "outputs": [],
      "source": [
        "#close enough backbone\n",
        "def build_fcn(input_shape, backbone):\n",
        "    \"\"\"Helper function to build an FCN model.\n",
        "\n",
        "    Arguments:\n",
        "        backbone (Model): A backbone network\n",
        "            such as ResNetv2 or v1\n",
        "        n_classes (int): Number of object classes\n",
        "            including background.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    features = backbone(inputs)\n",
        "\n",
        "    main_feature = features[0]\n",
        "    features = features[1:]\n",
        "    out_features = [main_feature]\n",
        "    feature_size = 8\n",
        "    size = 2\n",
        "    N = 1\n",
        "    # other half of the features pyramid\n",
        "    # including upsampling or downsampling to resize the\n",
        "    # feature maps to the dimensions\n",
        "    # equal to 1/4 the image size\n",
        "    for feature in features:\n",
        "        postfix = \"fcn_\" + str(feature_size)\n",
        "        feature = conv_layer(feature,\n",
        "                             filters=256,\n",
        "                             use_maxpool=False,\n",
        "                             postfix=postfix)\n",
        "        postfix = postfix + \"_up2d\"\n",
        "        if feature.shape[1] != main_feature.shape[1]:\n",
        "            feature = UpSampling2D(size=(main_feature.shape[1] // feature.shape[1], main_feature.shape[2] // feature.shape[2]),\n",
        "                               interpolation='bilinear',\n",
        "                               name=postfix)(feature)\n",
        "        else:\n",
        "            feature = Conv2D(256, kernel_size=3, strides=1, padding=\"same\", name=postfix)(feature)\n",
        "        out_features.append(feature)\n",
        "        size = size * 2\n",
        "        feature_size = feature_size * 2\n",
        "    \n",
        "    #out_features = out_features[:-2] \n",
        "    #print(out_features)\n",
        "    # concatenate all upsampled features\n",
        "    # concatenate all upsampled features\n",
        "    x = Concatenate()(out_features)\n",
        "    y = tf.identity(x)\n",
        "    # binary segmentation branch\n",
        "    # perform 2 additional feature extraction \n",
        "    # and upsampling\n",
        "    x = tconv_layer(x, 256, postfix=\"up_x2\")\n",
        "    x = tconv_layer(x, 256, postfix=\"up_x4\")\n",
        "\n",
        "    # generate the pixel-wise classifier\n",
        "    x = Conv2DTranspose(filters=1,\n",
        "                    kernel_size=1,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    name=\"pre_activation_x\")(x)\n",
        "    x = Activation('sigmoid', name=\"classification_x\")(x)\n",
        "\n",
        "    #instance segmentation branch\n",
        "    y = tconv_layer(y, 256, postfix=\"up_y2\")\n",
        "    y = tconv_layer(y, 256, postfix=\"up_y4\")\n",
        "    \n",
        "    y = Conv2DTranspose(filters=N,\n",
        "                    kernel_size=1,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    name=\"pre_activation_y\")(y)\n",
        "    y = Activation('sigmoid', name=\"classification_y\")(y)\n",
        "\n",
        "    #product\n",
        "    z = tf.multiply(x, y)\n",
        "\n",
        "    model = Model(inputs, z, name=\"fcn\")\n",
        "\n",
        "    return model   "
      ],
      "id": "609d0456"
    },
    {
      "cell_type": "code",
      "source": [
        "#def lanenet_loss(binary_seg_logits, binary_label, instance_seg_logits, instance_label):\n",
        "def lanenet_loss(binary_seg_logits, binary_label):\n",
        "    # Binary segmentation loss\n",
        "    binary_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=binary_seg_logits, labels=binary_label))\n",
        "    \n",
        "    # Instance segmentation loss\n",
        "    # calculate instance nums\n",
        "    #unique_labels, unique_id, counts = tf.unique_with_counts(correct_label)\n",
        "    #counts = tf.cast(counts, tf.float32)\n",
        "    #num_instances = tf.size(unique_labels)\n",
        "\n",
        "    # calculate instance pixel embedding mean vec\n",
        "    #segmented_sum = tf.unsorted_segment_sum(\n",
        "    #    reshaped_pred, unique_id, num_instances)\n",
        "    #mu = tf.div(segmented_sum, tf.reshape(counts, (-1, 1)))\n",
        "    #mu_expand = tf.gather(mu, unique_id)\n",
        "    \n",
        "    # Combined loss\n",
        "    #total_loss = binary_loss + self.alpha*instance_loss\n",
        "    \n",
        "    return binary_loss"
      ],
      "metadata": {
        "id": "goyRvnqRmRkO"
      },
      "id": "goyRvnqRmRkO",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "31a74ab6",
        "outputId": "d92fe53f-84db-47dc-f81f-45ca061dd6b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_path:  ./data/gt_binary_image\n",
            "label_path:  ./data/gt_binary_image\n",
            "label_path:  ./data/gt_binary_image\n",
            "label_path:  ./data/gt_binary_image\n",
            "label_path:  ./data/gt_binary_image\n",
            "label_path:  ./data/gt_binary_image\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/6 [===================>..........] - ETA: 1:24 - loss: 1.8013"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-1f61caf465bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "# Generate the training data and labels\n",
        "data_path = \"./data/image\"\n",
        "label_path = \"./data/gt_binary_image\"\n",
        "image_names = keys = [\"000\"+str(i)+\".png\" for i in range(6)]\n",
        "train_data, train_label = data_generation(keys, data_path, label_path)\n",
        "\n",
        "# Build the model\n",
        "input_shape = train_data[0].shape\n",
        "backbone = build_resnet(input_shape, n_layers=2, version=2, n=6)\n",
        "model = build_fcn(input_shape, backbone)\n",
        "#model.summary()\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(lr=1e-4)\n",
        "model.compile(optimizer=optimizer, loss=lanenet_loss)\n",
        "\n",
        "# Train the model\n",
        "batch_size = 1\n",
        "epochs = 10\n",
        "history = model.fit(train_data, train_label, batch_size=batch_size, epochs=epochs)\n"
      ],
      "id": "31a74ab6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fe5905a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, UpSampling2D, Concatenate, Activation\n",
        "\n",
        "def build_fcn(input_shape, backbone):\n",
        "    \"\"\"Helper function to build an FCN model.\n",
        "\n",
        "    Arguments:\n",
        "        backbone (Model): A backbone network\n",
        "            such as ResNetv2 or v1\n",
        "        n_classes (int): Number of object classes\n",
        "            including background.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    features = backbone(inputs)\n",
        "\n",
        "    main_feature = features[0]\n",
        "    features = features[1:]\n",
        "    out_features = [main_feature]\n",
        "    feature_size = 8\n",
        "    size = 2\n",
        "    N = 1\n",
        "    # other half of the features pyramid\n",
        "    # including upsampling or downsampling to resize the\n",
        "    # feature maps to the dimensions\n",
        "    # equal to 1/4 the image size\n",
        "    for i, feature in enumerate(features):\n",
        "        postfix = \"fcn_\" + str(feature_size)\n",
        "        feature = conv_layer(feature,\n",
        "                             filters=256,\n",
        "                             use_maxpool=False,\n",
        "                             postfix=postfix)\n",
        "        postfix = postfix + \"_up2d\"\n",
        "        if feature.shape[1] != main_feature.shape[1]:\n",
        "            feature = UpSampling2D(size=(main_feature.shape[1] // feature.shape[1], main_feature.shape[2] // feature.shape[2]),\n",
        "                               interpolation='bilinear',\n",
        "                               name=postfix)(feature)\n",
        "        else:\n",
        "            feature = Conv2D(256, kernel_size=3, strides=1, padding=\"same\", name=postfix)(feature)\n",
        "        out_features.append(feature)\n",
        "        size = size * 2\n",
        "        feature_size = feature_size * 2\n",
        "\n",
        "        if i == 1:  # resize the fourth feature tensor\n",
        "            shape_diff = main_feature.shape[1] - feature.shape[1]\n",
        "            resized_feature = UpSampling2D(size=(1, 1 + shape_diff), interpolation=\"bilinear\")(feature)\n",
        "            out_features[3] = resized_feature\n",
        "\n",
        "    # concatenate all upsampled features\n",
        "    x = Concatenate()(out_features)\n",
        "    y = tf.identity(x)\n",
        "    # binary segmentation branch\n",
        "    # perform 2 additional feature extraction \n",
        "    # and upsampling\n",
        "    x = tconv_layer(x, 256, postfix=\"up_x2\")\n",
        "    x = tconv_layer(x, 256, postfix=\"up_x4\")\n",
        "\n",
        "    # generate the pixel-wise classifier\n",
        "    x = Conv2DTranspose(filters=1,\n",
        "                    kernel_size=1,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    name=\"pre_activation\")(x)\n",
        "    x = Activation('sigmoid', name=\"classification\")(x)\n",
        "\n",
        "    #instance segmentation branch\n",
        "    y = tconv_layer(y, 256, postfix=\"up_x2\")\n",
        "    y = tconv_layer(y, 256, postfix=\"up_x4\")\n",
        "    \n",
        "    y = Conv2DTranspose(filters=N,\n",
        "                    kernel_size=1,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    name=\"pre_activation\")(y)\n",
        "    y = Activation('sigmoid', name=\"classification\")(y)\n",
        "\n",
        "    #product\n",
        "    z = tf.multiply(x, y)\n",
        "\n",
        "    model = Model(inputs, z, name=\"fcn\")\n",
        "\n",
        "    return model   "
      ],
      "id": "9fe5905a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "434b3bab",
        "outputId": "08945d01-b7a4-40ac-9666-2f95c926d0cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234545216/234545216 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-780401057d17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet152V2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_fcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-32632b34f4b0>\u001b[0m in \u001b[0;36mbuild_fcn\u001b[0;34m(input_shape, backbone)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# feature maps to the dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# equal to 1/4 the image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mpostfix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"fcn_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         feature = conv_layer(feature,\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/keras_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot iterate over a scalar.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    409\u001b[0m                 \u001b[0;34m\"Cannot iterate over a Tensor with unknown first dimension.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             )\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot iterate over a Tensor with unknown first dimension."
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications.resnet_v2 import ResNet152V2\n",
        "\n",
        "input_shape = (720, 1280, 3)\n",
        "backbone = ResNet152V2(include_top=False, input_shape=input_shape)\n",
        "\n",
        "model = build_fcn(input_shape, backbone)"
      ],
      "id": "434b3bab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af80cbee"
      },
      "outputs": [],
      "source": [
        "#close enough backbone\n",
        "def build_fcn(input_shape, backbone):\n",
        "    \"\"\"Helper function to build an FCN model.\n",
        "\n",
        "    Arguments:\n",
        "        backbone (Model): A backbone network\n",
        "            such as ResNetv2 or v1\n",
        "        n_classes (int): Number of object classes\n",
        "            including background.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    features = backbone(inputs)\n",
        "\n",
        "    main_feature = features[0]\n",
        "    features = features[1:]\n",
        "    out_features = [main_feature]\n",
        "    feature_size = 8\n",
        "    size = 2\n",
        "    # other half of the features pyramid\n",
        "    # including upsampling or downsampling to resize the\n",
        "    # feature maps to the dimensions\n",
        "    # equal to 1/4 the image size\n",
        "    for feature in features:\n",
        "        postfix = \"fcn_\" + str(feature_size)\n",
        "        feature = conv_layer(feature,\n",
        "                             filters=256,\n",
        "                             use_maxpool=False,\n",
        "                             postfix=postfix)\n",
        "        postfix = postfix + \"_up2d\"\n",
        "        if feature.shape[1] != main_feature.shape[1]:\n",
        "            feature = UpSampling2D(size=(main_feature.shape[1] // feature.shape[1], main_feature.shape[2] // feature.shape[2]),\n",
        "                               interpolation='bilinear',\n",
        "                               name=postfix)(feature)\n",
        "        else:\n",
        "            feature = Conv2D(256, kernel_size=3, strides=1, padding=\"same\", name=postfix)(feature)\n",
        "        out_features.append(feature)\n",
        "        size = size * 2\n",
        "        feature_size = feature_size * 2\n",
        "\n",
        "    # concatenate all upsampled features\n",
        "    x = Concatenate()(out_features)\n",
        "    # perform 2 additional feature extraction \n",
        "    # and upsampling\n",
        "    x = tconv_layer(x, 256, postfix=\"up_x2\")\n",
        "    x = tconv_layer(x, 256, postfix=\"up_x4\")\n",
        "    # generate the pixel-wise classifier\n",
        "    x = Conv2DTranspose(filters=1,\n",
        "                    kernel_size=1,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    name=\"pre_activation\")(x)\n",
        "    x = Activation('sigmoid', name=\"classification\")(x)\n",
        "\n",
        "    model = Model(inputs, x, name=\"fcn\")\n",
        "\n",
        "    return model"
      ],
      "id": "af80cbee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58cb50b4"
      },
      "outputs": [],
      "source": [],
      "id": "58cb50b4"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}