{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8467dcf6"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
        "from tensorflow.keras.layers import Add\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from tensorflow.keras.layers import Activation, Input\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import BatchNormalization, Concatenate\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Softmax, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "import skimage\n",
        "from skimage.io import imread\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "id": "8467dcf6"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loguru"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig1JI8U8sGXx",
        "outputId": "f45724ca-b2d4-4b7c-ca68-846dd362ecef"
      },
      "id": "ig1JI8U8sGXx",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.9/dist-packages (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import loguru\n",
        "LOG = loguru.logger"
      ],
      "metadata": {
        "id": "cpXnJVJnspiA"
      },
      "id": "cpXnJVJnspiA",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURE_DIM = 4\n",
        "delta_v = 0.5\n",
        "delta_d = 0.5\n",
        "param_var = 0.5\n",
        "param_dist = 0.5\n",
        "param_reg = 0.5\n",
        "num_images = 4\n",
        "data_path = \"./data/image\"\n",
        "label_path = \"./data/gt_binary_image\"\n",
        "MIN_AREA_THRESHOLD = 100\n",
        "DBSCAN_EPS = 0.35\n",
        "DBSCAN_MIN_SAMPLES = 1000\n",
        "_color_map = [np.array([255, 0, 0]),\n",
        "              np.array([0, 255, 0]),\n",
        "              np.array([0, 0, 255]),\n",
        "              np.array([125, 125, 0]),\n",
        "              np.array([0, 125, 125]),\n",
        "              np.array([125, 0, 125]),\n",
        "              np.array([50, 100, 50]),\n",
        "              np.array([100, 50, 100])]"
      ],
      "metadata": {
        "id": "tL1hQ1imulGM"
      },
      "id": "tL1hQ1imulGM",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d81d6f94"
      },
      "outputs": [],
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    Arguments:\n",
        "        inputs (tensor): Input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): Activation name\n",
        "        batch_normalization (bool): Whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    Returns:\n",
        "        x (tensor): Tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "id": "d81d6f94"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4e096c02"
      },
      "outputs": [],
      "source": [
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): Shape of input image tensor\n",
        "        depth (int): Number of core convolutional layers\n",
        "        num_classes (int): Number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = Add()([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # feature maps\n",
        "    outputs = features_pyramid(x, n_layers)\n",
        "    \n",
        "\n",
        "    # instantiate model\n",
        "    name = 'ResNet%dv1' % (depth)\n",
        "    model = Model(inputs=inputs,\n",
        "                  outputs=outputs,\n",
        "                  name=name)\n",
        "    return model"
      ],
      "id": "4e096c02"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "20a8cbb3"
      },
      "outputs": [],
      "source": [
        "def resnet_v2(input_shape, depth, n_layers=4):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): Shape of input image tensor\n",
        "        depth (int): Number of core convolutional layers\n",
        "        num_classes (int): Number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = Add()([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    # 1st feature map layer\n",
        "\n",
        "    # main feature maps (160, 120)\n",
        "    # succeeding feature maps are scaled down by\n",
        "    # 2, 4, 8\n",
        "    outputs = features_pyramid(x, n_layers)\n",
        "\n",
        "    # instantiate model.\n",
        "    name = 'ResNet%dv2' % (depth)\n",
        "    model = Model(inputs=inputs,\n",
        "                  outputs=outputs,\n",
        "                  name=name)\n",
        "    return model"
      ],
      "id": "20a8cbb3"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1a2f7765"
      },
      "outputs": [],
      "source": [
        "def features_pyramid(x, n_layers):\n",
        "    \"\"\"Generate features pyramid from the output of the \n",
        "    last layer of a backbone network (e.g. ResNetv1 or v2)\n",
        "\n",
        "    Arguments:\n",
        "        x (tensor): Output feature maps of a backbone network\n",
        "        n_layers (int): Number of additional pyramid layers\n",
        "\n",
        "    Return:\n",
        "        outputs (list): Features pyramid \n",
        "    \"\"\"\n",
        "    outputs = [x]\n",
        "    conv = AveragePooling2D(pool_size=2, name='pool1')(x)\n",
        "    outputs.append(conv)\n",
        "    prev_conv = conv\n",
        "    n_filters = 512\n",
        "\n",
        "    # additional feature map layers\n",
        "    for i in range(n_layers - 1):\n",
        "        postfix = \"_layer\" + str(i+2)\n",
        "        conv = conv_layer(prev_conv,\n",
        "                          n_filters,\n",
        "                          kernel_size=3,\n",
        "                          strides=2,\n",
        "                          use_maxpool=False,\n",
        "                          postfix=postfix)\n",
        "        outputs.append(conv)\n",
        "        prev_conv = conv\n",
        "\n",
        "    return outputs"
      ],
      "id": "1a2f7765"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5002d251"
      },
      "outputs": [],
      "source": [
        "def build_resnet(input_shape,\n",
        "                 n_layers=4,\n",
        "                 version=2,\n",
        "                 n=6):\n",
        "    \"\"\"Build a resnet as backbone\n",
        "\n",
        "    # Arguments:\n",
        "        input_shape (list): Input image size and channels\n",
        "        n_layers (int): Number of feature layers \n",
        "        version (int): Supports ResNetv1 and v2 but v2 by default\n",
        "        n (int): Determines number of ResNet layers\n",
        "                 (Default is ResNet50)\n",
        "\n",
        "    # Returns\n",
        "        model (Keras Model)\n",
        "\n",
        "    \"\"\"\n",
        "    # computed depth from supplied model parameter n\n",
        "    if version == 1:\n",
        "        depth = n * 6 + 2\n",
        "    elif version == 2:\n",
        "        depth = n * 9 + 2\n",
        "\n",
        "    # model name, depth and version\n",
        "    # input_shape (h, w, 3)\n",
        "    if version==1:\n",
        "        model = resnet_v1(input_shape=input_shape,\n",
        "                          depth=depth,\n",
        "                          n_layers=n_layers)\n",
        "    else:\n",
        "        model = resnet_v2(input_shape=input_shape,\n",
        "                          depth=depth,\n",
        "                          n_layers=n_layers)\n",
        "    return model"
      ],
      "id": "5002d251"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "48f2c2a4"
      },
      "outputs": [],
      "source": [
        "def data_generation(keys, data_path, label_path):\n",
        "    \"\"\"Generate train data: images and \n",
        "    segmentation ground truth labels \n",
        "\n",
        "    Arguments:\n",
        "        keys (array): Randomly sampled keys\n",
        "            (key is image filename)\n",
        "\n",
        "    Returns:\n",
        "        x (tensor): Batch of images\n",
        "        y (tensor): Batch of pixel-wise categories\n",
        "    \"\"\"\n",
        "    # a batch of images\n",
        "    x = []\n",
        "    # and their corresponding segmentation masks\n",
        "    y = []\n",
        "\n",
        "    for i, key in enumerate(keys):\n",
        "        # images are assumed to be stored \n",
        "        # in self.args.data_path\n",
        "        # key is the image filename \n",
        "        image_path = os.path.join(data_path, key)\n",
        "       # print('image_path: ', image_path)\n",
        "        # load the image using OpenCV\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "        # resize the image to (640, 480)\n",
        "        #print(image)\n",
        "        image = cv2.resize(image, (640, 480), interpolation=cv2.INTER_LINEAR)\n",
        "        # convert the image from BGR to RGB format\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        # append image to the list\n",
        "        x.append(image)\n",
        "        # and its corresponding label (segmentation mask)\n",
        "       # print('label_path: ', label_path)\n",
        "        label = os.path.join(label_path, key)\n",
        "        label = skimage.img_as_float(imread(label))\n",
        "        # resize the label to (640, 480)\n",
        "        label = cv2.resize(label, (640, 480), interpolation=cv2.INTER_NEAREST)\n",
        "        y.append(label)\n",
        "\n",
        "    return np.array(x), np.array(y)"
      ],
      "id": "48f2c2a4"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "93abef89"
      },
      "outputs": [],
      "source": [
        "def conv_layer(inputs,\n",
        "               filters=32,\n",
        "               kernel_size=3,\n",
        "               strides=1,\n",
        "               use_maxpool=True,\n",
        "               postfix=None,\n",
        "               activation=None):\n",
        "    \"\"\"Helper function to build Conv2D-BN-ReLU layer\n",
        "        with optional MaxPooling2D.\n",
        "    \"\"\"\n",
        "\n",
        "    x = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               strides=strides,\n",
        "               kernel_initializer='he_normal',\n",
        "               name=\"conv_\"+postfix,\n",
        "               padding='same')(inputs)\n",
        "    x = BatchNormalization(name=\"bn_\"+postfix)(x)\n",
        "    x = Activation('relu', name='relu_'+postfix)(x)\n",
        "    if use_maxpool:\n",
        "        x = MaxPooling2D(name='pool'+postfix)(x)\n",
        "    return x"
      ],
      "id": "93abef89"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2dd160f4"
      },
      "outputs": [],
      "source": [
        "def tconv_layer(inputs,\n",
        "                filters=32,\n",
        "                kernel_size=3,\n",
        "                strides=2,\n",
        "                postfix=None):\n",
        "    \"\"\"Helper function to build Conv2DTranspose-BN-ReLU \n",
        "        layer\n",
        "    \"\"\"\n",
        "    x = Conv2DTranspose(filters=filters,\n",
        "                        kernel_size=kernel_size,\n",
        "                        strides=strides,\n",
        "                        padding='same',\n",
        "                        kernel_initializer='he_normal',\n",
        "                        name='tconv_'+postfix)(inputs)\n",
        "    x = BatchNormalization(name=\"bn_\"+postfix)(x)\n",
        "    x = Activation('relu', name='relu_'+postfix)(x)\n",
        "    return x"
      ],
      "id": "2dd160f4"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "609d0456"
      },
      "outputs": [],
      "source": [
        "#close enough backbone\n",
        "def build_fcn(input_shape, backbone):\n",
        "    \"\"\"Helper function to build an FCN model.\n",
        "\n",
        "    Arguments:\n",
        "        backbone (Model): A backbone network\n",
        "            such as ResNetv2 or v1\n",
        "        n_classes (int): Number of object classes\n",
        "            including background.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    features = backbone(inputs)\n",
        "\n",
        "    main_feature = features[0]\n",
        "    features = features[1:]\n",
        "    out_features = [main_feature]\n",
        "    feature_size = 8\n",
        "    size = 2\n",
        "    N = 4\n",
        "    # other half of the features pyramid\n",
        "    # including upsampling or downsampling to resize the\n",
        "    # feature maps to the dimensions\n",
        "    # equal to 1/4 the image size\n",
        "    for feature in features:\n",
        "        postfix = \"fcn_\" + str(feature_size)\n",
        "        feature = conv_layer(feature,\n",
        "                             filters=256,\n",
        "                             use_maxpool=False,\n",
        "                             postfix=postfix)\n",
        "        postfix = postfix + \"_up2d\"\n",
        "        if feature.shape[1] != main_feature.shape[1]:\n",
        "            feature = UpSampling2D(size=(main_feature.shape[1] // feature.shape[1], main_feature.shape[2] // feature.shape[2]),\n",
        "                               interpolation='bilinear',\n",
        "                               name=postfix)(feature)\n",
        "        else:\n",
        "            feature = Conv2D(256, kernel_size=3, strides=1, padding=\"same\", name=postfix)(feature)\n",
        "        out_features.append(feature)\n",
        "        size = size * 2\n",
        "        feature_size = feature_size * 2\n",
        "    \n",
        "    #out_features = out_features[:-2] \n",
        "    #print(out_features)\n",
        "    # concatenate all upsampled features\n",
        "    # concatenate all upsampled features\n",
        "    x = Concatenate()(out_features)\n",
        "    y = tf.identity(x)\n",
        "    # binary segmentation branch\n",
        "    # perform 2 additional feature extraction \n",
        "    # and upsampling\n",
        "    x = tconv_layer(x, 256, postfix=\"up_x2\")\n",
        "    x = tconv_layer(x, 256, postfix=\"up_x4\")\n",
        "\n",
        "    # generate the pixel-wise classifier\n",
        "    x = Conv2DTranspose(filters=1,\n",
        "                    kernel_size=1,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    name=\"pre_activation_x\")(x)\n",
        "    x = Activation('sigmoid', name=\"classification_x\")(x)\n",
        "\n",
        "    #instance segmentation branch\n",
        "    y = tconv_layer(y, 256, postfix=\"up_y2\")\n",
        "    y = tconv_layer(y, 256, postfix=\"up_y4\")\n",
        "    \n",
        "    y = Conv2DTranspose(filters=FEATURE_DIM,\n",
        "                    kernel_size=1,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    name=\"pre_activation_y\")(y)\n",
        "    y = Activation('sigmoid', name=\"classification_y\")(y)\n",
        "    #y = tf.round(y)\n",
        "    #y = tf.cast(y, tf.int32)\n",
        "\n",
        "\n",
        "    #product\n",
        "    #z = tf.multiply(x, y)\n",
        "\n",
        "    #model = Model(inputs, z, name=\"fcn\")\n",
        "    model = Model(inputs, (x,y), name=\"fcn\")\n",
        "\n",
        "    return model   "
      ],
      "id": "609d0456"
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_loss(binary_seg_logits, binary_label):\n",
        "    binary_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=binary_seg_logits, labels=binary_label))\n",
        "    return binary_loss"
      ],
      "metadata": {
        "id": "goyRvnqRmRkO"
      },
      "id": "goyRvnqRmRkO",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lanenet_loss(correct_label, instance_seg_logits):\n",
        "    _, rows, cols = np.shape(correct_label)\n",
        "    \n",
        "    correct_label = tf.reshape(correct_label, [-1])\n",
        "    instance_seg_logits = tf.reshape(instance_seg_logits, (-1, 4))\n",
        "\n",
        "    # Get the unique values and their counts\n",
        "    unique_labels, unique_id, counts = tf.unique_with_counts(tf.reshape(correct_label, [-1]))\n",
        "    segmented_sum = tf.math.unsorted_segment_sum(instance_seg_logits, correct_label, FEATURE_DIM)\n",
        "    #tf.print(segmented_sum)\n",
        "    counts = tf.cast(counts, tf.float32)\n",
        "\n",
        "    num_instances = tf.size(unique_labels)\n",
        "    mu = tf.math.divide(segmented_sum, tf.reshape(counts, (-1,1)))\n",
        "    mu_expand = tf.gather(mu, unique_id)\n",
        "\n",
        "    instance_seg_logits = tf.cast(instance_seg_logits, tf.float32)\n",
        "    distance = tf.norm(tf.subtract(mu_expand, instance_seg_logits), axis=1, ord=1)\n",
        "    distance = tf.subtract(distance, delta_v)\n",
        "    distance = tf.clip_by_value(distance, 0., distance)\n",
        "    distance = tf.square(distance)\n",
        "\n",
        "    l_var = tf.math.unsorted_segment_sum(distance, unique_id, num_instances)\n",
        "    l_var = tf.math.divide(l_var, counts)\n",
        "    l_var = tf.math.reduce_sum(l_var)\n",
        "    l_var = tf.math.divide(l_var, tf.cast(num_instances, tf.float32))\n",
        "\n",
        "    mu_interleaved_rep = tf.tile(mu, [num_instances, 1])\n",
        "    mu_band_rep = tf.tile(mu, [1, num_instances])\n",
        "    mu_band_rep = tf.reshape(mu_band_rep,(num_instances * num_instances, FEATURE_DIM))\n",
        "\n",
        "    mu_diff = tf.math.subtract(mu_band_rep, mu_interleaved_rep)\n",
        "\n",
        "    intermediate_tensor = tf.math.reduce_sum(tf.math.abs(mu_diff), axis=1)\n",
        "    zero_vector = tf.zeros((1,), dtype=tf.float32)\n",
        "    bool_mask = tf.math.not_equal(intermediate_tensor, zero_vector)\n",
        "    mu_diff_bool = tf.boolean_mask(mu_diff, bool_mask)\n",
        "\n",
        "    mu_norm = tf.norm(mu_diff_bool, axis=1, ord=1)\n",
        "    mu_norm = tf.math.subtract(2. * delta_d, mu_norm)\n",
        "    mu_norm = tf.clip_by_value(mu_norm, 0., mu_norm)\n",
        "    mu_norm = tf.math.square(mu_norm)\n",
        "\n",
        "    l_dist = tf.math.reduce_mean(mu_norm)\n",
        "\n",
        "    l_reg = tf.reduce_mean(tf.norm(mu, axis=1, ord=1))\n",
        "\n",
        "    param_scale = 1.\n",
        "    l_var = param_var * l_var\n",
        "    l_dist = param_dist * l_dist\n",
        "    l_reg = param_reg * l_reg\n",
        "\n",
        "    loss = param_scale * (l_var + l_dist + l_reg)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "EgYmV09HpwgL"
      },
      "id": "EgYmV09HpwgL",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31a74ab6",
        "outputId": "86127418-c5ff-4f30-a69f-4d9a50faf4df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train label shape:  (4, 480, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 202s 43s/step - loss: 3.4748 - classification_x_loss: 0.6906 - classification_y_loss: 1.6585\n",
            "1/1 [==============================] - 48s 48s/step\n"
          ]
        }
      ],
      "source": [
        "image_names = keys = [\"000\"+str(i)+\".png\" for i in range(num_images)]\n",
        "train_data, train_label = data_generation(keys, data_path, label_path)\n",
        "\n",
        "print(\"train label shape: \", train_label.shape)\n",
        "# Define the image shape\n",
        "image_shape = (480, 640)\n",
        "\n",
        "# Create an empty numpy array to store the images\n",
        "images = np.empty((num_images, *image_shape), dtype=np.int32)\n",
        "\n",
        "# Fill in the numpy array with random integer values between 0 and 4\n",
        "for i in range(num_images):\n",
        "    images[i] = np.random.randint(0, FEATURE_DIM, size=image_shape, dtype=np.int32)\n",
        "\n",
        "# Build the model\n",
        "input_shape = train_data[0].shape\n",
        "backbone = build_resnet(input_shape, n_layers=2, version=2, n=6)\n",
        "model = build_fcn(input_shape, backbone)\n",
        "#model.summary()\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(lr=1e-4)\n",
        "#binary_label = train_label  # replace with actual binary label\n",
        "#instance_label = train_label  # replace with actual instance label\n",
        "model.compile(optimizer=optimizer, loss=[binary_loss,lanenet_loss])\n",
        "\n",
        "#print(\"images: \", np.shape(images))\n",
        "# Train the model\n",
        "batch_size = 1\n",
        "epochs = 1\n",
        "history = model.fit(train_data, (train_label, images), batch_size=batch_size, epochs=epochs)\n",
        "binary_seg_ret, instance_seg_ret =  model.predict(train_data)"
      ],
      "id": "31a74ab6"
    },
    {
      "cell_type": "code",
      "source": [
        "#print(instance_seg_ret.shape)\n",
        "binary_seg_ret_reshaped = np.reshape(binary_seg_ret[0], (480, 640))\n",
        "instance_seg_ret_reshaped = np.reshape(instance_seg_ret[0], (480, 640,4))\n",
        "print(instance_seg_ret_reshaped.shape)\n",
        "# Print the new shape\n",
        "#print(binary_seg_ret_reshaped.shape)\n",
        "postprocess(binary_seg_ret_reshaped,instance_seg_ret_reshaped )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30bC_oTykDNI",
        "outputId": "99fdf000-d6e1-4f52-a28e-e3e3141f2faa"
      },
      "id": "30bC_oTykDNI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(480, 640, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _morphological_process(image, kernel_size=5):\n",
        "    \"\"\"\n",
        "    morphological process to fill the hole in the binary segmentation result\n",
        "    :param image:\n",
        "    :param kernel_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if len(image.shape) == 3:\n",
        "        raise ValueError('Binary segmentation result image should be a single channel image')\n",
        "\n",
        "    if image.dtype is not np.uint8:\n",
        "        image = np.array(image, np.uint8)\n",
        "\n",
        "    kernel = cv2.getStructuringElement(shape=cv2.MORPH_ELLIPSE, ksize=(kernel_size, kernel_size))\n",
        "\n",
        "    # close operation fille hole\n",
        "    closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
        "\n",
        "    return closing"
      ],
      "metadata": {
        "id": "DO2tVvqCfy3j"
      },
      "id": "DO2tVvqCfy3j",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _connect_components_analysis(image):\n",
        "    \"\"\"\n",
        "    connect components analysis to remove the small components\n",
        "    :param image:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if len(image.shape) == 3:\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        gray_image = image\n",
        "\n",
        "    return cv2.connectedComponentsWithStats(gray_image, connectivity=8, ltype=cv2.CV_32S)"
      ],
      "metadata": {
        "id": "VLsVUWf_viXQ"
      },
      "id": "VLsVUWf_viXQ",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KpJHWLGyNYuc"
      },
      "id": "KpJHWLGyNYuc",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_lane_embedding_feats(binary_seg_ret, instance_seg_ret):\n",
        "        \"\"\"\n",
        "        get lane embedding features according the binary seg result\n",
        "        :param binary_seg_ret:\n",
        "        :param instance_seg_ret:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        idx = np.where(binary_seg_ret == 255)\n",
        "        lane_embedding_feats = instance_seg_ret[idx]\n",
        "        lane_coordinate = np.vstack((idx[1], idx[0])).transpose()\n",
        "\n",
        "        assert lane_embedding_feats.shape[0] == lane_coordinate.shape[0]\n",
        "\n",
        "        ret = {\n",
        "            'lane_embedding_feats': lane_embedding_feats,\n",
        "            'lane_coordinates': lane_coordinate\n",
        "        }\n",
        "\n",
        "        return ret"
      ],
      "metadata": {
        "id": "NIHh0YBox392"
      },
      "id": "NIHh0YBox392",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _embedding_feats_dbscan_cluster(embedding_image_feats):\n",
        "        \"\"\"\n",
        "        dbscan cluster\n",
        "        :param embedding_image_feats:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        db = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES)\n",
        "        try:\n",
        "            features = StandardScaler().fit_transform(embedding_image_feats)\n",
        "            db.fit(features)\n",
        "        except Exception as err:\n",
        "            LOG.error(err)\n",
        "            ret = {\n",
        "                'origin_features': None,\n",
        "                'cluster_nums': 0,\n",
        "                'db_labels': None,\n",
        "\n",
        "                'unique_labels': None,\n",
        "                'cluster_center': None\n",
        "            }\n",
        "            return ret\n",
        "        db_labels = db.labels_\n",
        "        unique_labels = np.unique(db_labels)\n",
        "\n",
        "        num_clusters = len(unique_labels)\n",
        "        cluster_centers = db.components_\n",
        "\n",
        "        ret = {\n",
        "            'origin_features': features,\n",
        "            'cluster_nums': num_clusters,\n",
        "            'db_labels': db_labels,\n",
        "            'unique_labels': unique_labels,\n",
        "            'cluster_center': cluster_centers\n",
        "        }\n",
        "\n",
        "        return ret"
      ],
      "metadata": {
        "id": "C6his5SSzPXX"
      },
      "id": "C6his5SSzPXX",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_lane_embedding_feats(binary_seg_ret, instance_seg_ret):\n",
        "    \"\"\"\n",
        "    get lane embedding features according the binary seg result\n",
        "    :param binary_seg_ret:\n",
        "    :param instance_seg_ret:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    idx = np.where(binary_seg_ret == 255)\n",
        "    lane_embedding_feats = instance_seg_ret[idx]\n",
        "    lane_coordinate = np.vstack((idx[1], idx[0])).transpose()\n",
        "\n",
        "    assert lane_embedding_feats.shape[0] == lane_coordinate.shape[0]\n",
        "\n",
        "    ret = {\n",
        "        'lane_embedding_feats': lane_embedding_feats,\n",
        "        'lane_coordinates': lane_coordinate\n",
        "    }\n",
        "\n",
        "    return ret\n"
      ],
      "metadata": {
        "id": "ptHjcppjzcbr"
      },
      "id": "ptHjcppjzcbr",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_lane_feats_cluster(binary_seg_result, instance_seg_result):\n",
        "        \"\"\"\n",
        "        :param binary_seg_result:\n",
        "        :param instance_seg_result:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # get embedding feats and coords\n",
        "        get_lane_embedding_feats_result = _get_lane_embedding_feats(\n",
        "            binary_seg_ret=binary_seg_result,\n",
        "            instance_seg_ret=instance_seg_result\n",
        "        )\n",
        "\n",
        "        # dbscan cluster\n",
        "        dbscan_cluster_result = _embedding_feats_dbscan_cluster(\n",
        "            embedding_image_feats=get_lane_embedding_feats_result['lane_embedding_feats']\n",
        "        )\n",
        "\n",
        "        mask = np.zeros(shape=[binary_seg_result.shape[0], binary_seg_result.shape[1], 3], dtype=np.uint8)\n",
        "        db_labels = dbscan_cluster_result['db_labels']\n",
        "        unique_labels = dbscan_cluster_result['unique_labels']\n",
        "        coord = get_lane_embedding_feats_result['lane_coordinates']\n",
        "\n",
        "        if db_labels is None:\n",
        "            return None, None\n",
        "\n",
        "        lane_coords = []\n",
        "        for index, label in enumerate(unique_labels.tolist()):\n",
        "            if label == -1:\n",
        "                continue\n",
        "            idx = np.where(db_labels == label)\n",
        "            pix_coord_idx = tuple((coord[idx][:, 1], coord[idx][:, 0]))\n",
        "            mask[pix_coord_idx] = _color_map[index]\n",
        "            lane_coords.append(coord[idx])\n",
        "\n",
        "        return mask, lane_coords"
      ],
      "metadata": {
        "id": "E7lbTRGPo-Ji"
      },
      "id": "E7lbTRGPo-Ji",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(binary_seg_result, instance_seg_result=None,\n",
        "                    min_area_threshold=100, source_image=None,\n",
        "                    with_lane_fit=True, data_source='tusimple'):\n",
        "        \"\"\"\n",
        "        :param binary_seg_result:\n",
        "        :param instance_seg_result:\n",
        "        :param min_area_threshold:\n",
        "        :param source_image:\n",
        "        :param with_lane_fit:\n",
        "        :param data_source:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # convert binary_seg_result\n",
        "        binary_seg_result = np.array(binary_seg_result * 255, dtype=np.uint8)\n",
        "\n",
        "        # apply image morphology operation to fill in the hold and reduce the small area\n",
        "        morphological_ret = _morphological_process(binary_seg_result, kernel_size=5)\n",
        "        connect_components_analysis_ret = _connect_components_analysis(image=morphological_ret)\n",
        "\n",
        "        labels = connect_components_analysis_ret[1]\n",
        "        stats = connect_components_analysis_ret[2]\n",
        "        for index, stat in enumerate(stats):\n",
        "            if stat[4] <= min_area_threshold:\n",
        "                idx = np.where(labels == index)\n",
        "                morphological_ret[idx] = 0\n",
        "\n",
        "        mask_image, lane_coords = apply_lane_feats_cluster(\n",
        "            binary_seg_result=morphological_ret,\n",
        "            instance_seg_result=instance_seg_result\n",
        "        )\n",
        "\n",
        "        if mask_image is None:\n",
        "          return {\n",
        "              'mask_image': None,\n",
        "              'fit_params': None,\n",
        "              'source_image': None,\n",
        "              }\n",
        "        \n",
        "        if not with_lane_fit:\n",
        "          tmp_mask = cv2.resize(\n",
        "              mask_image,\n",
        "              dsize=(source_image.shape[1], source_image.shape[0]),\n",
        "              interpolation=cv2.INTER_NEAREST\n",
        "              )\n",
        "        \n",
        "        source_image = cv2.addWeighted(source_image, 0.6, tmp_mask, 0.4, 0.0, dst=source_image)\n",
        "        return {\n",
        "            'mask_image': mask_image,\n",
        "            'fit_params': None,\n",
        "            'source_image': source_image,\n",
        "            }\n",
        "        \n",
        "         # lane line fit\n",
        "        fit_params = []\n",
        "        src_lane_pts = []  # lane pts every single lane\n",
        "        for lane_index, coords in enumerate(lane_coords):\n",
        "            if data_source == 'tusimple':\n",
        "                tmp_mask = np.zeros(shape=(720, 1280), dtype=np.uint8)\n",
        "                tmp_mask[tuple((np.int_(coords[:, 1] * 720 / 256), np.int_(coords[:, 0] * 1280 / 512)))] = 255\n",
        "            else:\n",
        "                raise ValueError('Wrong data source now only support tusimple')\n",
        "            tmp_ipm_mask = cv2.remap(\n",
        "                tmp_mask,\n",
        "                self._remap_to_ipm_x,\n",
        "                self._remap_to_ipm_y,\n",
        "                interpolation=cv2.INTER_NEAREST\n",
        "            )\n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "UUEPJdbAzygp"
      },
      "id": "UUEPJdbAzygp",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PSgD4No41sVM"
      },
      "id": "PSgD4No41sVM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}